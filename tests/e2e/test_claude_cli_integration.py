#!/usr/bin/env python3
"""End-to-end integration tests with actual Claude CLI processes."""

import asyncio
import json
import os
import shutil
import subprocess
import tempfile
import time
import uuid
from pathlib import Path
from typing import Optional

import pytest

from apex.core.lmdb_mcp import LMDBMCP
from apex.core.memory import MemoryPatterns
from apex.core.process_manager import ClaudeProcess, ProcessManager
from apex.core.stream_parser import StreamParser
from apex.mcp.lmdb_server import LMDBServer
from apex.agents.prompts import AgentPrompts
from apex.types import AgentType, ProjectConfig


class TestClaudeCLIIntegration:
    """End-to-end integration tests with actual Claude CLI."""

    @pytest.fixture
    async def temp_environment(self):
        """Set up temporary environment for testing."""
        # Create temporary directory for test data
        temp_dir = Path(tempfile.mkdtemp(prefix="apex_e2e_"))
        lmdb_path = temp_dir / "test_apex.db"
        mcp_config_path = temp_dir / "mcp_config.json"
        
        # Create MCP configuration pointing to our test LMDB
        mcp_config = {
            "mcpServers": {
                "lmdb": {
                    "command": "python",
                    "args": ["-m", "apex.mcp.lmdb_server"],
                    "env": {
                        "LMDB_PATH": str(lmdb_path),
                        "LMDB_MAP_SIZE": "1073741824"  # 1GB for testing
                    }
                }
            }
        }
        
        # Write MCP config file
        with open(mcp_config_path, 'w') as f:
            json.dump(mcp_config, f, indent=2)
        
        # Set up LMDB and memory patterns
        lmdb_manager = LMDBMCP(lmdb_path)
        memory = MemoryPatterns(lmdb_manager)
        
        yield {
            "temp_dir": temp_dir,
            "lmdb_path": lmdb_path, 
            "mcp_config_path": mcp_config_path,
            "lmdb_manager": lmdb_manager,
            "memory": memory
        }
        
        # Cleanup
        lmdb_manager.close()
        shutil.rmtree(temp_dir, ignore_errors=True)

    def test_claude_cli_availability(self):
        """Test that Claude CLI is available in the environment."""
        try:
            result = subprocess.run(
                ["claude", "--version"], 
                capture_output=True, 
                text=True, 
                timeout=10
            )
            assert result.returncode == 0, f"Claude CLI not available: {result.stderr}"
            print(f"âœ… Claude CLI available: {result.stdout.strip()}")
        except (subprocess.TimeoutExpired, FileNotFoundError) as e:
            pytest.skip(f"Claude CLI not available: {e}")

    async def test_lmdb_mcp_server_standalone(self, temp_environment):
        """Test LMDB MCP server can start independently."""
        env = temp_environment
        
        # Create LMDB server process
        server_env = os.environ.copy()
        server_env.update({
            "LMDB_PATH": str(env["lmdb_path"]),
            "LMDB_MAP_SIZE": "1073741824"
        })
        
        # Start LMDB MCP server
        server_process = subprocess.Popen(
            ["python", "-m", "apex.mcp.lmdb_server"],
            env=server_env,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        try:
            # Send MCP initialization
            init_request = {
                "jsonrpc": "2.0",
                "id": 1,
                "method": "initialize",
                "params": {
                    "protocolVersion": "2024-11-05",
                    "capabilities": {},
                    "clientInfo": {"name": "test-client", "version": "1.0.0"}
                }
            }
            
            server_process.stdin.write(json.dumps(init_request) + "\n")
            server_process.stdin.flush()
            
            # Read response with timeout
            response_line = None
            start_time = time.time()
            while time.time() - start_time < 5:  # 5 second timeout
                if server_process.poll() is not None:
                    break
                try:
                    line = server_process.stdout.readline()
                    if line.strip():
                        response_line = line.strip()
                        break
                except:
                    pass
                time.sleep(0.1)
            
            assert response_line is not None, "No response from LMDB MCP server"
            
            response = json.loads(response_line)
            assert response["id"] == 1
            assert "result" in response
            assert "capabilities" in response["result"]
            
            print("âœ… LMDB MCP server responds correctly")
            
        finally:
            server_process.terminate()
            try:
                server_process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                server_process.kill()

    @pytest.mark.asyncio
    async def test_claude_process_creation(self, temp_environment):
        """Test creating Claude process with MCP configuration."""
        env = temp_environment
        
        # Create simple test prompt
        prompt = "You are a test agent. Respond with 'Test successful' and exit."
        
        # Create Claude process
        claude_process = ClaudeProcess(
            agent_type=AgentType.SUPERVISOR,
            prompt=prompt,
            mcp_config=env["mcp_config_path"],
            model="claude-sonnet-4-20250514"
        )
        
        # Verify command construction
        expected_args = [
            "claude", "-p", prompt, 
            "--output-format", "stream-json",
            "--model", "claude-sonnet-4-20250514",
            "--mcp-config", str(env["mcp_config_path"]),
            "--allowedTools", claude_process._get_allowed_tools(),
            "--max-turns", "50",
            "--verbose"
        ]
        
        assert claude_process.command == expected_args
        print("âœ… Claude process command constructed correctly")

    @pytest.mark.asyncio
    async def test_agent_communication_via_lmdb(self, temp_environment):
        """Test agent communication through LMDB without actual Claude CLI."""
        env = temp_environment
        
        # Create test project
        project_config = ProjectConfig(
            project_id=str(uuid.uuid4()),
            name="test-project",
            description="Test project for E2E testing",
            tech_stack=["Python"],
            project_type="Library",
            features=["testing"]
        )
        
        # Create project in memory
        await env["memory"].create_project(project_config)
        
        # Simulate supervisor creating a task
        task_data = {
            "description": "Create basic Python module structure",
            "assigned_to": AgentType.CODER.value,
            "priority": "high",
            "requirements": ["Create __init__.py", "Add basic module structure"],
            "estimated_time": "30 minutes"
        }
        
        task_id = await env["memory"].create_task(
            project_id=project_config.project_id,
            **task_data
        )
        
        print(f"âœ… Task created: {task_id}")
        
        # Verify task is in pending state
        pending_tasks = await env["memory"].get_pending_tasks(
            project_config.project_id, 
            AgentType.CODER
        )
        
        assert len(pending_tasks) == 1
        assert pending_tasks[0]["task_id"] == task_id
        assert pending_tasks[0]["status"] == "pending"
        
        print("âœ… Task correctly stored and retrievable")
        
        # Simulate coder picking up and completing the task
        completion_result = {
            "analysis": "Created Python module structure",
            "files_created": ["__init__.py", "module.py"],
            "tests_added": ["test_module.py"],
            "completion_notes": "Basic structure implemented successfully"
        }
        
        success = await env["memory"].complete_task(
            task_id, 
            completion_result
        )
        
        assert success
        print("âœ… Task completed successfully")
        
        # Verify task status updated
        task_status = await env["memory"].get_task_status(task_id)
        assert task_status["status"] == "completed"
        assert task_status["result"] == completion_result
        
        print("âœ… Task status correctly updated")

    @pytest.mark.asyncio  
    async def test_stream_parser_with_mock_output(self, temp_environment):
        """Test stream parser with mock Claude CLI output."""
        env = temp_environment
        
        # Create session ID for testing
        session_id = str(uuid.uuid4())
        agent_id = "supervisor_1"
        
        # Create stream parser
        parser = StreamParser(
            mcp=env["lmdb_manager"],
            agent_id=agent_id,
            session_id=session_id
        )
        
        # Mock Claude CLI JSON output lines
        mock_output_lines = [
            '{"type": "system", "id": "sys_1", "content": "Agent started successfully"}',
            '{"type": "assistant", "id": "msg_1", "content": "I am ready to help with the task."}',
            '{"type": "tool_use", "id": "tool_1", "name": "mcp__lmdb__read", "input": {"key": "/projects/test/tasks"}}',
            '{"type": "tool_result", "id": "result_1", "tool_use_id": "tool_1", "content": "[]"}',
            '{"type": "assistant", "id": "msg_2", "content": "No pending tasks found. Waiting for new assignments."}',
        ]
        
        # Process each line
        events = []
        for line in mock_output_lines:
            parsed_events = parser.parse_line(line)
            events.extend(parsed_events)
        
        # Verify events were parsed correctly
        assert len(events) == 5
        
        # Check event types
        assert events[0].event_type == "system"
        assert events[1].event_type == "assistant"  
        assert events[2].event_type == "tool_use"
        assert events[3].event_type == "tool_result"
        assert events[4].event_type == "assistant"
        
        # Verify tool use event details
        tool_event = events[2]
        assert tool_event.data["name"] == "mcp__lmdb__read"
        assert tool_event.data["input"]["key"] == "/projects/test/tasks"
        
        print("âœ… Stream parser correctly parsed mock Claude output")

    @pytest.mark.asyncio
    async def test_process_manager_lifecycle(self, temp_environment):
        """Test process manager with mock processes."""
        env = temp_environment
        
        # Create project config
        project_config = ProjectConfig(
            project_id=str(uuid.uuid4()),
            name="test-project", 
            description="Test project",
            tech_stack=["Python"],
            project_type="Library",
            features=["testing"]
        )
        
        # Create process manager
        process_manager = ProcessManager(
            project_config=project_config,
            lmdb_path=env["lmdb_path"]
        )
        
        try:
            # Test agent status (no processes started yet)
            status = process_manager.get_agent_status()
            assert len(status) == 0
            print("âœ… Initial agent status is empty")
            
            # Note: We don't actually start Claude processes here as they require
            # real Claude CLI authentication and would make the test too complex.
            # Instead we verify the process manager can track processes correctly.
            
            # Simulate adding process info manually for testing
            test_process_info = {
                "type": AgentType.SUPERVISOR.value,
                "status": "running",
                "start_time": time.time(),
                "pid": 12345,  # Mock PID
                "running": True
            }
            
            # Add to internal tracking (for testing purposes)
            process_manager.processes["supervisor_1"] = test_process_info
            
            # Check status again
            status = process_manager.get_agent_status()
            assert len(status) == 1
            assert "supervisor_1" in status
            assert status["supervisor_1"]["type"] == "supervisor"
            
            print("âœ… Process manager correctly tracks agent status")
            
        finally:
            # Cleanup
            await process_manager.stop_all_agents()

    @pytest.mark.asyncio
    async def test_complete_workflow_simulation(self, temp_environment):
        """Test complete workflow simulation without actual Claude CLI."""
        env = temp_environment
        
        print("ðŸš€ Starting Complete E2E Workflow Simulation")
        print("=" * 60)
        
        # 1. Create project
        project_config = ProjectConfig(
            project_id=str(uuid.uuid4()),
            name="calculator-e2e",
            description="Calculator app for E2E testing",
            tech_stack=["Python", "CLI"],
            project_type="CLI Tool", 
            features=["add", "subtract", "multiply", "divide"]
        )
        
        await env["memory"].create_project(project_config)
        print(f"âœ… Project created: {project_config.name}")
        
        # 2. Supervisor creates initial tasks
        supervisor_tasks = [
            {
                "description": "Analyze requirements and create project structure",
                "assigned_to": AgentType.CODER.value,
                "priority": "high",
                "requirements": ["Create Calculator class", "Design API", "Plan tests"]
            },
            {
                "description": "Review code quality and security",
                "assigned_to": AgentType.ADVERSARY.value, 
                "priority": "medium",
                "requirements": ["Code review", "Security analysis", "Performance check"]
            }
        ]
        
        task_ids = []
        for task_data in supervisor_tasks:
            task_id = await env["memory"].create_task(
                project_id=project_config.project_id,
                **task_data
            )
            task_ids.append(task_id)
        
        print(f"âœ… Created {len(task_ids)} initial tasks")
        
        # 3. Verify task distribution
        coder_tasks = await env["memory"].get_pending_tasks(
            project_config.project_id, 
            AgentType.CODER
        )
        adversary_tasks = await env["memory"].get_pending_tasks(
            project_config.project_id,
            AgentType.ADVERSARY
        )
        
        assert len(coder_tasks) == 1
        assert len(adversary_tasks) == 1
        print("âœ… Tasks correctly distributed to agents")
        
        # 4. Simulate coder completing first task
        coder_task_id = coder_tasks[0]["task_id"]
        coder_result = {
            "analysis": "Requirements analyzed, project structure planned",
            "files_planned": ["calculator.py", "cli.py", "tests/test_calculator.py"],
            "api_design": {
                "Calculator": ["add", "subtract", "multiply", "divide"],
                "CLI": ["parse_args", "main"]
            },
            "next_steps": ["Implement Calculator class", "Add unit tests", "Create CLI interface"]
        }
        
        await env["memory"].complete_task(coder_task_id, coder_result)
        print("âœ… Coder completed analysis task")
        
        # 5. Simulate adversary completing review task  
        adversary_task_id = adversary_tasks[0]["task_id"]
        adversary_result = {
            "security_review": "No security issues found in design",
            "performance_notes": "Consider input validation for division by zero",
            "quality_recommendations": [
                "Add type hints",
                "Include docstrings", 
                "Add error handling"
            ],
            "approval_status": "approved_with_recommendations"
        }
        
        await env["memory"].complete_task(adversary_task_id, adversary_result)
        print("âœ… Adversary completed review task")
        
        # 6. Verify workflow state
        completed_tasks = []
        for task_id in task_ids:
            task_status = await env["memory"].get_task_status(task_id)
            if task_status["status"] == "completed":
                completed_tasks.append(task_status)
        
        assert len(completed_tasks) == 2
        print(f"âœ… All {len(completed_tasks)} tasks completed successfully")
        
        # 7. Check project progress
        project_status = await env["memory"].get_project_status(project_config.project_id)
        assert project_status is not None
        print("âœ… Project status tracking working")
        
        # 8. Test memory patterns for code storage
        sample_code = """
class Calculator:
    def add(self, a: float, b: float) -> float:
        return a + b
        
    def subtract(self, a: float, b: float) -> float:
        return a - b
"""
        
        await env["memory"].store_code(
            project_id=project_config.project_id,
            file_path="calculator.py",
            content=sample_code,
            author="coder_agent"
        )
        
        stored_code = await env["memory"].get_code(
            project_config.project_id,
            "calculator.py"
        )
        
        assert stored_code is not None
        assert "class Calculator" in stored_code["content"]
        print("âœ… Code storage and retrieval working")
        
        print("\nðŸŽ¯ E2E Workflow Simulation Results:")
        print("   âœ… Project creation and configuration")
        print("   âœ… Task creation and assignment")
        print("   âœ… Agent communication via LMDB")
        print("   âœ… Task completion and status tracking") 
        print("   âœ… Code storage and retrieval")
        print("   âœ… Memory pattern integration")
        print("   âœ… Multi-agent workflow coordination")
        print("\nðŸš€ APEX E2E workflow fully functional!")


if __name__ == "__main__":
    # Run tests if executed directly
    pytest.main([__file__, "-v", "-s"])